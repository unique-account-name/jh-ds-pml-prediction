---
title: "Practical Machine Learning: Prediction Assignment Writeup"
author: "R. P. Ruiz"
date: "May 8, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
DEBUG <- FALSE
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.

The Human Activity Recognition group at the Pontifical Catholic University of Rio de Janeiro has loaned us some the data they've collected (http://groupware.les.inf.puc-rio.br/har).

We're going to attempt to train a model for predicting which of 5 possible activities their experiment's subjects have performed and recorded.


## Dataset Preparation

Import raw data and standardize missing information (NA)

```{r cache=TRUE}
# which values should be treated as NAs when imported?
na_synonyms <- c( "", "NA", "NULL", "#DIV/0!" )
exercise_test <- read.csv( "../data/pml-testing.csv", na.strings = na_synonyms )
exercise_train <- read.csv( "../data/pml-training.csv", na.strings = na_synonyms )

dim( exercise_train )
dim( exercise_test )
```

##Drop Irrelevent Columns and NA Values

There is a considerable amount of data here, over 19K observations containing 160 different fields.  Many of them contain NA values, so we'll take the easy way out and omit them.  If this were a more rigorous analysis, we'd consider imputation to replace missing data

```{r}
# notice that test set doesn't contain target column 'classe'...
print( "classe" %in% names( exercise_test ) )
# ...while training does
print( "classe" %in% names( exercise_train ) )

# columns to omit, including time series data. 
columns_not_needed <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window", "problem_id" )

# is.na( exercise_test ) provides cell by cell calc of NA presence
# colSums essentially tallies up the "1"s representing NAs.
columns_with_no_nas <- names( exercise_test[ , colSums( is.na( exercise_test ) ) == 0 ] )
if ( DEBUG ) columns_with_no_nas

# drop all column names not needed: if it's in the not needed list, then omit from the cleaned list
features <- columns_with_no_nas[ !( columns_with_no_nas %in% columns_not_needed ) ]
if ( DEBUG ) features

# get relevant subset of data, add in missing columns - we'll need 'classe' to train model
exercise_train <- exercise_train[ , c( features, "classe" ) ]
# ...and we'll need 'problem_id' to answer quiz at end of unit
exercise_test  <- exercise_test [ , c( features, "problem_id" ) ]
```

## Create Training & Validation Partitions
```{r}
invisible( library( caret ) )
set.seed( 05052016 )
inTrain <- createDataPartition( y = exercise_train$classe, p = 0.75, list = FALSE )
exercise_train_1  <- exercise_train[ inTrain, ]
exercise_train_2  <- exercise_train[ -inTrain, ]
dim( exercise_train_1 )
dim( exercise_train_2 )
```

## Perform Initial Classification Using `RPart`

We've whittled 160 columns down to 50-some, so now we'll take a quick brute-force look at a model created using all independent variables.

```{r cache=TRUE}
# build everthing-but-the-kitchen-sink model
model_all <- train( classe ~., data = exercise_train_1, method = "rpart" )
# dumps too much info to console
#summary( model_all )$coef

# get predictions
predictions_train <- predict( model_all, newdata = exercise_train_1 )
predictions_validate <- predict( model_all, newdata = exercise_train_2 )

# look at structure of predictions
if ( DEBUG ) str( predictions_train )
if ( DEBUG ) summary( predictions_train )

# check accuracy for training set...
confusionMatrix( exercise_train_1$classe, predictions_train )$table
confusionMatrix( exercise_train_1$classe, predictions_train )$overall[ 'Accuracy' ]

# ...and for validation set
confusionMatrix( exercise_train_2$classe, predictions_validate )$table
confusionMatrix( exercise_train_2$classe, predictions_validate )$overall[ 'Accuracy' ]
```

This level of accuracy is not good enough.  On to a different training algorithm

## Initial Classification Using RandomForest

Again, we'll attempt to build a model using all independent variables.

```{r cache=TRUE}
invisible( library( randomForest ) )

# create model, limit trees to 10 to start with
start_time <- proc.time()
model_rf <- randomForest( classe ~ ., data = exercise_train_1, do.trace = FALSE, ntrees = 10 )
proc.time() - start_time

varImpPlot( model_rf, main="Feature Importance" )
```

## Optimizing RandomForest Classification
It turns out that the brute force approach above (using all independent variables) takes a considerable amount of time to compute, so we'll take a look at the more important features, and rebuild the model using the top 10 most important variables. 

```{r cache=TRUE}
# calculate important features
important_features <- importance( model_rf, sort=TRUE )
importance_df <- data.frame( important_features, features )
sorted_importance <- importance_df[ order( importance_df$MeanDecreaseGini, decreasing = TRUE), ]
top_10_features <- sorted_importance[ 1:10, ]
if ( DEBUG ) top_10_features
# omit all other feature levels pulled in from larger list
top_10_feature_names <- factor( top_10_features$feature )
if ( DEBUG ) top_10_feature_names

start_time <- proc.time()
# this works in R console, but breaks in knitr, so use hand-built list instead...
#model_rf <- randomForest( classe ~ top_10_feature_names, data = exercise_train_1, do.trace = FALSE, ntrees = 10 )
model_rf <- randomForest( classe ~ roll_belt + yaw_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt + magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x + accel_dumbbell_y + roll_dumbbell, data = exercise_train_1, do.trace = FALSE, ntrees = 10 )
proc.time() - start_time

# training
predictions_rf_train <- predict( model_rf, newdata = exercise_train_1, type = "class" )
print( confusionMatrix( exercise_train_1$classe, predictions_rf_train )$table )
print( confusionMatrix( exercise_train_1$classe, predictions_rf_train )$overall[ 'Accuracy' ] )

# validation
predictions_rf_validation <- predict( model_rf, newdata = exercise_train_2, type = "class" )
print( confusionMatrix( exercise_train_2$classe, predictions_rf_validation )$table )
print( confusionMatrix( exercise_train_2$classe, predictions_rf_validation )$overall[ 'Accuracy' ] )

# test
predictions_rf_test <- predict( model_rf, newdata = exercise_test, type = "class" )
predictions_rf_test
```

Not only is the reduced feature set much faster, but it still maintains high accuracy even when tested against the validation set.

Normally, obtaining 99+ percent accuracy is *extremely unusual*, but we'll accept that, as my end goal is to predict the test set's activities as accurately as possible.

## Conclusion

100% accuracy on my 20 question quiz.  I pass!
